# Gradient descent

Gradient descent uses the derivatives $\partial f/\partial x_i$ to find a direction that reduces $f(x)$. The steepest direction, in which $f(x)$ decreases fastest, is given by the gradient $- \nabla f$ :
$$
x_{k+1}=x_{k}-s_{k}\nabla f(x_k)
$$
$s_k$ is the *step size* or the *learning rate*.

### The geometry of the gradient vector $\nabla f$

Let's just take a linear function $f(x,y)=2x+5y$, the gradient is the vector $\nabla f= \begin{bmatrix}{2}\\{5}\end{bmatrix}$, the Hessian matrix is $H=\bold{0}$ .

![image-20220117165218738](21.Gradient Descent.assets\image-20220117165218738.png)

The gradient is perpendicular to the plane $2x+5y=0$. The level direction is in the plane, $f=2x+5y$ is constant in this direction. 

### Hessian matrix

The Hessian tells us whether or not a function is convex. $F$  is convex if and only if $H$ is positive  semidefinite. $F$ is strictly convex if $H$ is positive definite. 

The linear function is convex, but not strictly convex.

### Examples

**Example 1:** $F(x)=\frac{1}{2}x^TSx-a^Tx$, where $S$ is positive definite, so that's a convex function. 

The gradient is $\nabla F =Sx-a$. The minimum occurs where $\nabla F =0$, so $x^*=S^{-1}a=agrmin F$.

$F_{min} =\frac{1}{2}(S^{-1}a)^TS(S^{-1}a)-a^T(S^{-1}a)=\frac{1}{2}a^TS^{-1}a-a^TS^{-1}a=-\frac{1}{2}a^TS^{-1}a$

**Example 2:** The determinant $F(x)=det(X)$ is a function of all $n^2$ variables $x_{ij}$.

$det(X)=x_{11}C_{11}+\cdots+x_{1n}C_{1n}$, where $C_{ij}$ is $x_{ij}$'s cofactor(代数余子式).

So the partial derivatives $\frac{\partial (det{X})}{\partial x_{ij}}=C_{ij}$.

**Example 3:** The logarithm of the determinant is a most remarkable function:
$$
L(X)=log(det\ X)
$$
The chain rule for $L=logF$ is :
$$
\frac{\partial L}{\partial x_{ij}}=\frac{\partial L}{\partial F}\frac{\partial F}{\partial x_{ij}}=\frac{1}{F}\frac{\partial F}{\partial x_{ij}}=\frac{1}{detX}C_{ij}=\frac{C_{ij}}{detX}
$$
this ratio of cofactors to determinant gives the $j,i$ entries of the inverse matrix $X^{-1}$.

$-log(det\ S)$ is a convex function of the entries of the positive definite matrix $S$.

## Exact line search and Backtracking

**Exact line search:** choose $s_k$ to make $f(x_{k+1})$ a minimum in search direction.

**Backtracking:** cut the step size in pieces and checks until you're satisfied with that step.

### An example with Zig-Zag

$f(x,y)=\frac{1}{2}(x^2+by^2)$, its gradient $\nabla f= \begin{bmatrix}{x}\\{by}\end{bmatrix}$, the minimum value of $f$ is zero. Starting from $(x_0,y_0)=(b,1)$ , with exact line search, we find these points:
$$
\begin{eqnarray}
x_k&=&b(\frac{b-1}{b+1})^k \\
y_k&=&(\frac{1-b}{1+b})^k \\
f(x_k,y_k)&=&(\frac{1-b}{1+b})^{2k}f(x_0,y_0)
\end{eqnarray}
$$
The figure below shows the convergence on a zig-zag path to the minimum of $f$.

![image-20220117165252483](21.Gradient Descent.assets\image-20220117165252483.png)